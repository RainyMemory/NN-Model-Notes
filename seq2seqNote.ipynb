{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 2, 16])\n"
     ]
    }
   ],
   "source": [
    "# Explore with the Embedding layer\n",
    "x = torch.tensor([[0, 2], [1, 1], [2, 0], [0, 0]], dtype=torch.long)\n",
    "input_cls_num = 3\n",
    "target_hid_dim = 16\n",
    "embed = torch.nn.Embedding(input_cls_num, target_hid_dim)\n",
    "embeded_seq = embed(x)\n",
    "print(embeded_seq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[-1.8788, -0.0331,  1.1378, -0.9246, -0.2182,  0.6418, -0.2694,\n          -0.4144, -1.2269,  1.4005,  0.1025,  0.9638,  0.6893,  1.1649,\n           1.9673, -1.3042],\n         [ 0.0096,  1.4777, -0.7478,  1.8785,  0.5272,  0.5485, -1.6951,\n          -0.3182, -0.4823, -0.0222, -1.4294,  1.0353,  0.8258,  1.1885,\n          -0.2497,  1.1072],\n         [-0.8538,  1.2686, -0.7242, -0.1459, -0.2994,  0.5838, -0.0340,\n           2.0798, -0.1340,  0.0988, -0.3270, -0.1567, -1.5562, -0.5270,\n           0.4882, -0.3418],\n         [-1.8788, -0.0331,  1.1378, -0.9246, -0.2182,  0.6418, -0.2694,\n          -0.4144, -1.2269,  1.4005,  0.1025,  0.9638,  0.6893,  1.1649,\n           1.9673, -1.3042]]], grad_fn=<EmbeddingBackward>)\n"
     ]
    }
   ],
   "source": [
    "x_tp = torch.tensor([[0, 1, 2, 0]], dtype=torch.long)\n",
    "embeded_tp = embed(x_tp)\n",
    "print(embeded_tp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The shape of bidirectional gru layer's output:  torch.Size([4, 2, 64])\nThe shape of simple gru layer's output:  torch.Size([4, 2, 32])\nWhen bidirectional, the shape of hidden states will be:  torch.Size([2, 2, 32])\nThus the simple one is:  torch.Size([1, 2, 32])\n"
     ]
    }
   ],
   "source": [
    "# Playing with GRU layer (bidirectional or not)\n",
    "# the input is in shape [seq len, batch size, embedding dim]\n",
    "input_seq = embeded_seq\n",
    "input_size = target_hid_dim\n",
    "hidden_size = 32\n",
    "# define the gru layers\n",
    "bi_gru = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, bidirectional=True)\n",
    "ow_gru = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, bidirectional=False)\n",
    "# every batch and rnn will only correspond to one hidden state, the length of sequence (num of tokens) and num of rnns decide the num of hidden state\n",
    "bi_output, bi_hidden_states = bi_gru(input_seq)\n",
    "ow_output, ow_hidden_states = ow_gru(input_seq)\n",
    "# the output will be in shape [seq len, batch size, hidden dim * num of networks]\n",
    "print(\"The shape of bidirectional gru layer's output: \", bi_output.shape)\n",
    "print(\"The shape of simple gru layer's output: \", ow_output.shape)\n",
    "# the hidden state is [num of networks, batch size, hidden dim]\n",
    "print(\"When bidirectional, the shape of hidden states will be: \", bi_hidden_states.shape)\n",
    "print(\"Thus the simple one is: \", ow_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 2, 4])\ntorch.Size([2, 8])\n"
     ]
    }
   ],
   "source": [
    "# Some tensor transform tricks\n",
    "tensor_a = torch.tensor([[[1, 1, 6, 6], [1, 1, 6, 6]], [[1, 1, 6, 6], [2, 4, 3, 9]]])\n",
    "print(tensor_a.shape)\n",
    "tensor_a_cat = torch.cat([tensor_a[0], tensor_a[1]], dim=-1)\n",
    "print(tensor_a_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 2, 8])\n"
     ]
    }
   ],
   "source": [
    "tensor_b = tensor_a_cat.repeat(2, 1, 1)\n",
    "print(tensor_b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[[[1],\n",
       "          [8],\n",
       "          [6],\n",
       "          [2]]]])"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "tensor_c.reshape((1,1,4,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([1, 4, 1])\ntorch.Size([1, 4])\ntorch.Size([1, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "tensor_c = torch.tensor([[[1], [8], [6], [2]]])\n",
    "print(tensor_c.shape)\n",
    "tensor_c_seq = tensor_c.squeeze(-1)\n",
    "print(tensor_c_seq.shape)\n",
    "tensor_c_unseq = tensor_c_seq.unsqueeze(0)\n",
    "print(tensor_c_unseq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([4, 1, 1])\ntensor([[0.3000],\n        [0.2000],\n        [0.4000],\n        [0.1000]])\ntorch.Size([4, 1])\n"
     ]
    }
   ],
   "source": [
    "# the usage of some matrix functions of pytorch\n",
    "attn_tensor = torch.tensor([[[.3]], [[.2]], [[.4]], [[.1]]], dtype=torch.float32)\n",
    "print(attn_tensor.shape)\n",
    "attn_tensor = attn_tensor.squeeze(-1)\n",
    "print(attn_tensor)\n",
    "print(attn_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tensor([[[0.2612, 0.2363, 0.2887, 0.2138]],\n\n        [[0.2612, 0.2363, 0.2887, 0.2138]]])\ntorch.Size([2, 1, 4])\ntensor([[[1., 2., 4.],\n         [2., 2., 3.],\n         [4., 3., 6.],\n         [3., 5., 6.]],\n\n        [[1., 2., 4.],\n         [2., 2., 3.],\n         [4., 3., 6.],\n         [3., 5., 6.]]])\ntorch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# prepare two matrices\n",
    "logits = F.softmax(attn.t())\n",
    "logits = torch.cat([logits, logits]).unsqueeze(-1)\n",
    "logits = logits.reshape(-1, logits.shape[-1], logits.shape[-2])\n",
    "print(logits)\n",
    "print(logits.shape)\n",
    "\n",
    "input_tensor = torch.tensor([[[1, 2, 4]], [[2, 2, 3]], [[4, 3, 6]], [[3, 5, 6]]], dtype=torch.float32)\n",
    "input_tensor = input_tensor.reshape(input_tensor.shape[1], input_tensor.shape[0], -1)\n",
    "input_tensor = torch.cat([input_tensor, input_tensor])\n",
    "print(input_tensor)\n",
    "print(input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([2, 1, 3])\ntensor([[[2.5300, 2.9302, 4.7687]],\n\n        [[2.5300, 2.9302, 4.7687]]])\n"
     ]
    }
   ],
   "source": [
    "# it should transform dim [a, b, c] and [a, c, d] to [a, b, d]\n",
    "answer = torch.bmm(logits, input_tensor)\n",
    "print(answer.shape)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqModel(torch.nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, output_dim, gru_num_layers=1, bidirectional=False, dropout=.3, hidden_layer=[64, 32, 64], max_seq_len=128):\n",
    "        super(Seq2SeqModel, self).__init__()\n",
    "        # store the variables\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        # thus the dim of ouputs can be aligned\n",
    "        self.hidden_dim = input_dim // 2 if bidirectional else input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.gru_num_layers = gru_num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "        # Encoder GRU\n",
    "        self.encoder = torch.nn.GRU(\n",
    "            input_size=self.input_dim, \n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=self.gru_num_layers, \n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Decoder GRU\n",
    "        self.decoder = torch.nn.GRU(\n",
    "            input_size=self.input_dim, \n",
    "            hidden_size=self.hidden_dim, \n",
    "            num_layers=self.gru_num_layers, \n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # The FFN to adjust the outputs\n",
    "        if hidden_layer and not len(hidden_layer) == 0:\n",
    "            # the dim is not changed through the two GRU layer\n",
    "            hidden_list = [torch.nn.Linear(self.input_dim, hidden_layer[0])]\n",
    "            for idx in range(len(hidden_layer) - 1):\n",
    "                hidden_list.append(torch.nn.Linear(hidden_layer[idx], hidden_layer[idx + 1]))\n",
    "            self.hidden_layer_list = torch.nn.ModuleList(hidden_list)\n",
    "            # init the weights\n",
    "            for layer in self.hidden_layer_list: \n",
    "                torch.nn.init.kaiming_normal_(layer.weight.data)\n",
    "            self.hidden_out_dim = hidden_layers[-1]\n",
    "        else:\n",
    "            self.hidden_layer_list = []\n",
    "            self.hidden_out_dim = self.input_dim\n",
    "        # Output layer\n",
    "        self.output = torch.nn.Linear(self.hidden_out_dim, self.output_dim)\n",
    "        torch.nn.init.kaiming_normal_(self.output.weight.data)\n",
    "        # Other functions\n",
    "        self.activate = torch.nn.ReLu()\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, prev_y, teacher=True):\n",
    "        # encode process\n",
    "        encode_x, encoder_hidden_info = self.encoder(x)\n",
    "        # decode process\n",
    "        if teacher:\n",
    "            decode_y, decoder_hidden_info = self.decoder(prev_y, encoder_hidden_info)\n",
    "        output = self.activate(decode_y)\n",
    "        # ffn process\n",
    "        for layer in self.hidden_layer_list:\n",
    "            output = layer(output)\n",
    "            output = self.activate(output)\n",
    "            output = self.dropout(output)\n",
    "        # output layer, get logits\n",
    "        output = self.output(decode_x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SqeEncoder(torch.nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, hidden_dim, bidirectional=False, dropout=.3):\n",
    "        super(Seq2SqeEncoder, self).__init__()\n",
    "        # keep the parameters\n",
    "        self.seq_len = seq_len\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # define encoder\n",
    "        self.encoder = torch.nn.GRU(\n",
    "            input_size=self.input_dim, \n",
    "            hidden_size=hidden_dim,\n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoder_output, hidden_states = self.encoder(x)\n",
    "        # if bidirectional, the dim of outputs will be doubled and we will get 2 hidden state\n",
    "        return encoder_output, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttention(torch.nn.Module):\n",
    "    def __init__(self, encoder_output_dim, decoder_hidden_dim):\n",
    "        super(Seq2SeqAttention, self).__init__()\n",
    "        # define attention mapping\n",
    "        self.attn = torch.nn.Linear(encoder_output_dim + decoder_hidden_dim, decoder_hidden_dim)\n",
    "        self.logits = torch.nn.Linear(decoder_hidden_dim, 1, bias=False)\n",
    "    \n",
    "    def forward(self, encoder_outputs, decoder_hidden_states):\n",
    "        # if the encoder & decoder are bidirectional, there will be two hidden states (for two RNN in different directions)\n",
    "        bidirectional = True if decoder_hidden_state.shape[0] == 2 else False\n",
    "        if bidirectional:\n",
    "            decoder_hidden_states = torch.cat([decoder_hidden_states[0], decoder_hidden_states[1]], dim=-1)\n",
    "        # attach the decoder's current hidden state to each encoder's output\n",
    "        decoder_hidden_states = decoder_hidden_states.repeat(encoder_outputs.shape[0], encoder_outputs.shape[1], 1)\n",
    "        attn_input = torch.cat([encoder_outputs, decoder_hidden_states], dim=-1)\n",
    "        # thus the attention input dim is [seq len, batch size, encoder_output_dim + decoder_hidden_state_dim]\n",
    "        attn = torch.tanh(self.attn(attn_input))\n",
    "        # for each seq, the logits layer ouputs one figure as its attention(weight) [seq len, batch size], then softmax will turn them into logits\n",
    "        attn_logits = F.softmax(self.logits(attn).squeeze(-1), dim=1)\n",
    "        return attn_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqDecoder(torch.nn.Module):\n",
    "    def __init__(self, seq_len, input_dim, hidden_dim, output_dim, attention_layer, bidirectional=False, dropout=.3):\n",
    "        super(Seq2SeqDecoder, self).__init__()\n",
    "        # keep the parameters\n",
    "        self.seq_len = seq_len\n",
    "        # for each input, the attention weight c is appended\n",
    "        self.input_dim = input_dim + hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.decoder_output_dim = hidden_dim * 2 if bidirectional else hidden_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        # create the decoder\n",
    "        self.attention = attention_layer\n",
    "        self.decoder = torch.nn.GRU(\n",
    "            input_size=self.input_dim, \n",
    "            hidden_size=self.hidden_dim,\n",
    "            bidirectional=bidirectional, \n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.output_layer = torch.nn.Linear(self.decoder_output_dim * 2, self.output_dim)\n",
    "\n",
    "    def forward(self, x, prev_hidden_states, encoder_outputs):\n",
    "        # in teacher mode, x can be prev_y\n",
    "        attn = self.attention(encoder_outputs=encoder_outputs, decoder_hidden_states=prev_hidden_states)\n",
    "        # transform the dim into [batch size, 1, seq len]\n",
    "        attn = attn.t().unsqueeze(1)\n",
    "        # encoder outputs [seq len, batch size, hidden dim]\n",
    "        weight_vec = torch.bmm(attn, encoder_outputs.reshape(encoder_outputs.shape[1], encoder_outputs.shape[0], -1))\n",
    "        # now weight vec [batch size, 1, hidden dim]\n",
    "        # input x, the embedding of prev output or ground truth [1, batch size, input dim]\n",
    "        weight_vec = weight_vec.reshape(weight_vec.shape[1], weight_vec.shape[0], -1)\n",
    "        # concate the embedding and weights with the batch size dim: [1, batch size, input dim + hidden dim]\n",
    "        decoder_input = torch.cat((x, weight_vec), dim = -1)\n",
    "        output, hidden_states = self.decoder(decoder_input, prev_hidden_states)\n",
    "        # deal with the output [1, batch size, output dim] -> [batch size, output dim + hidden dim]\n",
    "        output = self.output_layer(torch.cat([output, weight_vec], dim=-1).squeeze(0))\n",
    "        return output, hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2SeqAttnModel(torch.nn.Module):\n",
    "    def __init__(self, seq_len, num_class, embedding_dim=32, hidden_dim=32, bidirectional=False, dropout=.3):\n",
    "        super(Seq2SeqAttnModel, self).__init__()\n",
    "        device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "        self.encoder_hidden_dim = hidden_dim // 2 if bidirectional else hidden_dim\n",
    "        self.decoder_hidden_dim = encoder_hidden_dim\n",
    "        self.encoder = Seq2SqeEncoder(\n",
    "            seq_len=seq_len, \n",
    "            input_dim=embedding_dim, \n",
    "            hidden_dim=self.encoder_hidden_dim, \n",
    "            bidirectional=bidirectional, \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        self.attention = Seq2SeqAttention(self.encoder_hidden_dim, self.decoder_hidden_dim).to(device)\n",
    "        self.decoder = Seq2SeqDecoder(\n",
    "            seq_len=seq_len,\n",
    "            input_dim=embedding_dim, \n",
    "            hidden_dim=self.decoder_hidden_dim, \n",
    "            output_dim=num_class, \n",
    "            attention_layer=self.attention, \n",
    "            bidirectional=bidirectional, \n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, x, y, teacher=True):\n",
    "        # teacher mode\n",
    "        encoder_outputs, hidden_states = self.encoder(x)\n",
    "        predictions = []\n",
    "        if teacher:\n",
    "            for prev_label in y:\n",
    "                pred_y, decoder_hidden_states = self.decoder(x=prev_label, prev_hidden_states=hidden_states, encoder_outputs=encoder_outputs)\n",
    "                hidden_states = decoder_hidden_states\n",
    "                predictioins.append(pred_y)\n",
    "        return predictions"
   ]
  }
 ]
}