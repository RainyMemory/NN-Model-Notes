{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.8 64-bit ('envPytorch': conda)",
   "metadata": {
    "interpreter": {
     "hash": "82c25c09a0a8ff8dd25a51ab110a5b27617daf1cc0c39e1b255c757f044d7c3e"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# YOLOv3 网络构建与整体流程解析"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.autograd import Variable\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "source": [
    "## YOLOv3网络中使用的想对特殊层定义\n",
    "\n",
    "### EmptyLayer\n",
    "\n",
    "本层**不会**对输入的参数执行任何**改动操作**，其忠实的将传入参数传出。<br>\n",
    "其主要作用在于完成`router`以及`shortcut`两类操作，前者将当前输入与指定层输入拼接（`concat`），后者将输入与指定层输出相加（`addition`），他们都不改变上一层的输出内容，只是要将其和之前历史层输出进行组合操作，具体操作逻辑在`Darknet`中实现。<br>\n",
    "\n",
    "### YoLoLayer\n",
    "\n",
    "本层定义YOLO网络层，其主要对`Darknet`的最后输出进行操作，即按照YOLO定义逻辑，将输出转化为其标准输出格式：`Bounding box`加`Pred classes`（YOLOv3假定一个目标可以分属多个分类）加`Confidence`的形式，针对每个`grid`，做出与设定`anchor`数量相等的预测结果。\n",
    "\n",
    "- init：初始化\n",
    "\n",
    "在初始化过程中，定义预设锚点列表（`anchors`），输入图像尺寸（`image_size`），样本总类别数（`num_class`），最低`IoU`限制阈值（`ignore_thres`），是否进行CUDA调用（`cuda`），同时，有目标时贡献乘比参数`obj_scale`和无目标贡献乘比参数`noobj_scale`也在此处被定义。<br>\n",
    "此处使用最简单基本的`YOLO loss`计算方法：利用`MSEloss`（应用于`x`，`y`，`w`，`h`的长度直接损失）与`BCEloss`（应对利用`Sigmoid`激活后的二分类任务损失，如类别判断，`confidence`预估等）进行损失计算。\n",
    "\n",
    "- compute grid offset：计算当前各个`grid`偏移量\n",
    "\n",
    "在处理过程中，**感受野区域**会不断发生变化，在给定`grid_size`，即感受野区域大小后，每个`grid`提取所需的步长（`stride`），与在当前视野下`anchor`列表对应的缩放后大小都可得到计算（YOLOv3在每个给定感受野下根据给定的`anchor`列表给出预测）。\n",
    "\n",
    "- forward：前向传播\n",
    "\n",
    "此时输入内容`x`即为需要最后经由YOLO层转化到输出格式：根据**每个`grid`**求取如下内容：`x`，`y`的本单元格内**偏移量**（利用Sigmoid激活进入0~1后的`grid`内偏移率）。`w`，`h`位输出经由指数化后加入每个设定`anchor`锚点之中进行范围扰动，使得`Bounding box`能够有所伸展变化。`pred_class`由Sigmoid导出（YOLOv3放弃Softmax改对每个类应用Sigmoid，使得单目标同属多类成为可能，同时统一分类与`Confidence`的`loss`计算方式）。`conf`同样利用Sigmoid输出。<br>\n",
    "在输出格式构建完毕后，将其与`ground truth`传入`build targets`之中，利用当前输出格式编辑`ground truth`，使其与输出格式对齐，即可利用`ground truth`以及预设参数对当前预测结果`loss`进行计算，并将其输出。\n",
    "\n",
    "- bbox wh iou：通过给定Bounding Box的长宽计算其相交占比（IoU)\n",
    "\n",
    "此时不将`偏移量offset`纳入考虑，单纯利用当前`grid`中目标的`Bounding box`进行两者的重合度计算，本函数的目标单纯为判定哪一个给出`anchor`最适配当前`grid`中的目标，故我们不考虑`offset`，直接进行筛选。\n",
    "\n",
    "- build targets：根据`ground truth`建立对应的目标表示，由此来计算输出带来的损失`loss`\n",
    "\n",
    "首先，根据`ground truth`，我们能够构建`obj_mask`（有目标的`grid`及`anchor`）和与之相对的`noobj_mask`（无目标`grid`及`anchor`），他们对于损失的贡献不同。同时，针对每个目标，针对它所在的`grid`，只输出**最符合其形状**的`anchor`对应的`Bounding box`和`class`预测，由于我们的输出是针对每个`grid`的`x`，`y`偏移量，`w`，`h`抖动量，我们也需要对`ground truth`中的输出进行调整，通过使用log对数，移除floor（减去向下取整后，剩余小数位即为当前`grid`内的偏移量）等使其格式与YOLO网络输出对齐。<br>\n",
    "通过`pred_bbox`，`pred_class`，`target`，处理返回`class_mask`，`obj_mask`，`noobj_mask`，`tx`，`ty`，`tw`，`th`，`tcls`，`tconf`结果。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmptyLayer(nn.Module):\n",
    "    '''\n",
    "    For 'route' and 'shortcut'\n",
    "    We do the concate and activation when we run into these two kinds of layers\n",
    "    This EmptyLayer simply does nothing.\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(EmptyLayer, self).__init__()\n",
    "\n",
    "class YoLoLayer(nn.Module):\n",
    "    '''\n",
    "    The detection layer for the YoLo network\n",
    "    The 'Head' part\n",
    "    Key: the calculation of loss and the implemention of NMS\n",
    "    '''\n",
    "\n",
    "    def __init__(self, anchors, num_class, img_size=(416, 416), ignore_thres=0.5, cuda=False):\n",
    "        super(YoLoLayer, self).__init__()\n",
    "        # attributes on anchors\n",
    "        self.anchors = anchors\n",
    "        self.num_anchors = len(anchors)\n",
    "        # how many classes need to classifiy\n",
    "        self.num_class = num_class\n",
    "        # bounding boxed whit confidence below the threshold will be dropped\n",
    "        self.ignore_thres = ignore_thres\n",
    "        # the input image dim\n",
    "        self.img_dim = img_size[0]\n",
    "        # loss function used\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        self.bce_loss = nn.BCELoss()\n",
    "        # the scale factor of bounding box with objects in or not\n",
    "        self.obj_scale = 1\n",
    "        self.noobj_scale = 100\n",
    "        # how many grids are there in this image (split the image into how many parts)\n",
    "        self.grid_size = 0\n",
    "        # run the module on GPU or not\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def compute_grid_offsets(self, grid_size):\n",
    "        # in YoLo, we will get feature matrices with from different layers thus have different grid numbers\n",
    "        self.grid_size = grid_size\n",
    "        FloatTensor = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        # the distance between each grid (map onto the real input image)\n",
    "        self.stride = self.img_dim // self.grid_size\n",
    "        # the position of the current grid\n",
    "        self.grid_x = torch.arange(grid_size).repeat(grid_size, 1).view([1, 1, grid_size, grid_size]).type(FloatTensor)\n",
    "        self.grid_y = torch.arange(grid_size).repeat(grid_size, 1).t().view([1, 1, grid_size, grid_size]).type(FloatTensor)\n",
    "        # rescale the anchors with the same factors according to the grids\n",
    "        self.scaled_anchors = FloatTensor([(width / self.stride, height / self.stride) for width, height in self.anchors])\n",
    "        # the size of the current anchor\n",
    "        self.anchor_w = self.scaled_anchors[:, 0:1].view((1, self.num_anchors, 1, 1))\n",
    "        self.anchor_h = self.scaled_anchors[:, 1:2].view((1, self.num_anchors, 1, 1))\n",
    "    \n",
    "    def forward(self, x, targets=None, img_dim=None):\n",
    "        FloatTensor = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        LongTensor = torch.cuda.LongTensor if self.cuda else torch.LongTensor\n",
    "        ByteTensor = torch.cuda.ByteTensor if self.cuda else torch.ByteTensor\n",
    "        if img_dim is not None:\n",
    "            self.img_dim = img_dim\n",
    "        # get the size of the current batch\n",
    "        num_samples = x.size(0)\n",
    "        # x -> num_sample * channels(filters) * feature＿matrix_dim *　feature＿matrix_dim \n",
    "        grid_size = x.size(2)\n",
    "        # YoLo's output:\n",
    "        # num_samples: the current batch size\n",
    "        # num_anchors * (num_class + 5): for each type of anchor, give a prediction with:\n",
    "        #      classification results (num_class)\n",
    "        #      bounding box centre: x\n",
    "        #      bounding box centre: y\n",
    "        #      bounding box length\n",
    "        #      bounding box width\n",
    "        #      confidence\n",
    "        # grid_size * grid_size: how many grids in one image\n",
    "        # transform the dimension\n",
    "        prediction = (\n",
    "            x.view(num_samples, self.num_anchors, self.num_class + 5, grid_size, grid_size).permute(0, 1, 3, 4, 2).contiguous()\n",
    "        )\n",
    "        # predict the (num_class + 5)\n",
    "        # we compute the offset of the x,y in the current grid with the sigmoid function\n",
    "        x = torch.sigmoid(prediction[..., 0])\n",
    "        y = torch.sigmoid(prediction[..., 1])\n",
    "        w = prediction[..., 2]\n",
    "        h = prediction[..., 3]\n",
    "        pred_conf = torch.sigmoid(prediction[..., 4])\n",
    "        pred_class = torch.sigmoid(prediction[..., 5:])\n",
    "        # compute the attributes for each grid\n",
    "        if grid_size != self.grid_size:\n",
    "            self.compute_grid_offsets(grid_size)\n",
    "        # add the offset to bounding box according to the grid position\n",
    "        pred_bbox = FloatTensor(prediction[..., :4].shape)\n",
    "        pred_bbox[..., 0] = x.data + self.grid_x\n",
    "        pred_bbox[..., 1] = y.data + self.grid_y\n",
    "        # the width/height is computed with the exponent\n",
    "        pred_bbox[..., 2] = torch.exp(w.data) + self.anchor_w\n",
    "        pred_bbox[..., 3] = torch.exp(h.data) + self.anchor_h\n",
    "        # organize the output\n",
    "        output = torch.cat(\n",
    "            (\n",
    "                # map the bounding box to the position on the actual image\n",
    "                pred_bbox.view(num_samples, -1, 4) * self.stride,\n",
    "                pred_conf.view(num_samples, -1, 1),\n",
    "                pred_class.view(num_samples, -1, self.num_class)\n",
    "            ),\n",
    "            dim=-1\n",
    "        )\n",
    "        # no detection targets in the ground truth\n",
    "        if targets is None:\n",
    "            return output, 0\n",
    "        # calculate the loss\n",
    "        else:\n",
    "            class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf = self.build_targets(\n",
    "                pred_bbox=pred_bbox,\n",
    "                pred_class=pred_class,\n",
    "                target=targets\n",
    "            )\n",
    "            # the loss of bounding box center (x,y)\n",
    "            loss_x = self.mse_loss(x[obj_mask], tx[obj_mask])\n",
    "            loss_y = self.mse_loss(y[obj_mask], ty[obj_mask])\n",
    "            # the loss of the bounding box size\n",
    "            loss_w = self.mse_loss(w[obj_mask], tw[obj_mask])\n",
    "            loss_h = self.mse_loss(h[obj_mask], th[obj_mask])\n",
    "            # the loss of confidence (have objects or not)\n",
    "            loss_conf_obj = self.bce_loss(pred_conf[obj_mask], tconf[obj_mask])\n",
    "            loss_conf_noobj = self.bce_loss(pred_conf[noobj_mask], tconf[noobj_mask])\n",
    "            loss_conf = self.obj_scale * loss_conf_obj + self.noobj_scale * loss_conf_noobj\n",
    "            # the loss of class prediction\n",
    "            loss_cls = self.bce_loss(pred_class[obj_mask], tcls[obj_mask])\n",
    "            # total loss\n",
    "            total_loss = loss_x + loss_y + loss_w + loss_h + loss_conf + loss_cls\n",
    "            return output, total_loss\n",
    "    \n",
    "    def bbox_wh_iou(wh1, wh2):\n",
    "        wh2 = wh2.t()\n",
    "        # get the width/height from the anchor and the ground truth\n",
    "        w1, h1 = wh1[0], wh1[1]\n",
    "        w2, h2 = wh2[0], wh2[1]\n",
    "        # don not care about the offset, just check if the current anchor fit well with our ground truth\n",
    "        inter_area = torch.min(w1, w2) * torch.min(h1, h2)\n",
    "        union_area = (w1 * h1 + 1e-16) + w2 * h2 - inter_area\n",
    "        return inter_area / union_area\n",
    "    \n",
    "    def build_targets(self, pred_bbox, pred_class, target):\n",
    "        anchors=self.scaled_anchors\n",
    "        ignore_thres=self.ignore_thres\n",
    "        ByteTensor = torch.cuda.ByteTensor if self.cuda else torch.ByteTensor\n",
    "        FloatTensor = torch.cuda.FloatTensor if self.cuda else torch.FloatTensor\n",
    "        # batch size\n",
    "        nB = pred_bbox.size(0)\n",
    "        # anchor type\n",
    "        nA = pred_bbox.size(1)\n",
    "        # class num\n",
    "        nC = pred_class.size(-1)\n",
    "        # grid num\n",
    "        nG = pred_bbox.size(2)\n",
    "        # mark grids that have objects (or not)\n",
    "        obj_mask = ByteTensor(nB, nA, nG, nG).fill_(0)\n",
    "        noobj_mask = ByteTensor(nB, nA, nG, nG).fill_(1)\n",
    "        # the class of the object in the grid (if any)\n",
    "        class_mask = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        # target bound boxes\n",
    "        tx = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        ty = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        tw = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        th = FloatTensor(nB, nA, nG, nG).fill_(0)\n",
    "        tcls = FloatTensor(nB, nA, nG, nG, nC).fill_(0)\n",
    "        # use the number of grids to resize the bounding box to its 'real position' on the current feature matrix\n",
    "        target_boxes = target[:, 2:6] * nG\n",
    "        gxy = target_boxes[:, :2]\n",
    "        gwh = target_boxes[:, 2:]\n",
    "        # get the anchor with best iou according to the current ground truth\n",
    "        ious = torch.stack([self.bbox_wh_iou(anchor, gwh) for anchor in anchors])\n",
    "        best_ious, best_n = ious.max(0)\n",
    "        # Separate target values\n",
    "        b, target_labels = target[:, :2].long().t()\n",
    "        gx, gy = gxy.t()\n",
    "        gw, gh = gwh.t()\n",
    "        # get the target grids that have objects\n",
    "        gi, gj = gxy.long().t()\n",
    "        # set masks: mask the grids that have objects and label the correspond anchor type\n",
    "        obj_mask[b, best_n, gj, gi] = 1\n",
    "        noobj_mask[b, best_n, gj, gi] = 0\n",
    "        # Set noobj mask to zero where iou exceeds ignore threshold\n",
    "        # in order to punish the bounding boxes circle no object but have high confidence\n",
    "        for i, anchor_ious in enumerate(ious.t()):\n",
    "            noobj_mask[b[i], anchor_ious > ignore_thres, gj[i], gi[i]] = 0\n",
    "        # Coordinates, get rid of the floor so the result is the offset according to the current grid\n",
    "        tx[b, best_n, gj, gi] = gx - gx.floor()\n",
    "        ty[b, best_n, gj, gi] = gy - gy.floor()\n",
    "        # Width and height\n",
    "        # Using 1e-16 to avoid 0s\n",
    "        tw[b, best_n, gj, gi] = torch.log(gw / anchors[best_n][:, 0] + 1e-16)\n",
    "        th[b, best_n, gj, gi] = torch.log(gh / anchors[best_n][:, 1] + 1e-16)\n",
    "        # One-hot encoding of label, only the ground truth label will be marked as 1\n",
    "        tcls[b, best_n, gj, gi, target_labels] = 1\n",
    "        # Compute label correctness and iou at best anchor\n",
    "        class_mask[b, best_n, gj, gi] = (pred_class[b, best_n, gj, gi].argmax(-1) == target_labels).float()\n",
    "        # the grids have object have the confidence of 1.00\n",
    "        tconf = obj_mask.float()\n",
    "        return class_mask, obj_mask, noobj_mask, tx, ty, tw, th, tcls, tconf\n"
   ]
  },
  {
   "source": [
    "## YOLOv3：config文件读取配置\n",
    "\n",
    "### parse cfg：config文件解析\n",
    "\n",
    "按行读取所有内容，跳过注释等，根据格式将网络设计放入`dict`中，以供模型生成。\n",
    "\n",
    "### generate model：模型结构，基础层构建\n",
    "\n",
    "负责已有预设，简单层的定义，如卷积层，池化层等，同时统计每一层经过时`filter`的数量，以确保在梯度流的处理中，能够保证维度对齐。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_cfg(cfg_filepath):\n",
    "    cfg_file = open(cfg_filepath, mode='r')\n",
    "    # read the file line by line\n",
    "    contents = cfg_file.read().split('\\n')\n",
    "    # ignore all the empty lines\n",
    "    contents = [line for line in contents if len(line)>0]\n",
    "    # ignore all the muted information\n",
    "    contents = [line for line in contents if line[0] != '#']\n",
    "    contents = [line.rstrip().lstrip() for line in contents]\n",
    "    block = {}\n",
    "    block_list = []\n",
    "    for line in contents:\n",
    "        if line[0] is '[':\n",
    "            # not the first [net] block\n",
    "            if len(block) is not 0:\n",
    "                block_list.append(block)\n",
    "                block = {}\n",
    "            # delete the brakets\n",
    "            block['Type'] = line[1:-1].rstrip()\n",
    "        else:\n",
    "            k,v = line.split('=')\n",
    "            block[k.rstrip()] = v.lstrip()\n",
    "    block_list.append(block)\n",
    "    return block_list\n",
    "\n",
    "def generate_module(cfg_filepath):\n",
    "    block_list = parse_cfg(cfg_filepath)\n",
    "    # get hyperparameters from [net]\n",
    "    hyperparameters = block_list[0]\n",
    "    module_list = nn.ModuleList()\n",
    "    # give the list an initial input dim (3)\n",
    "    filters = 0\n",
    "    filter_list = [int(hyperparameters[\"channels\"])]\n",
    "    # create layers\n",
    "    for layer_index, block in enumerate(block_list[1:]):\n",
    "        module = nn.Sequential()\n",
    "        # check the type of the current block\n",
    "        # handle convolutional layers\n",
    "        if block['Type'] == 'convolutional':\n",
    "            # extract the important parameters\n",
    "            try:\n",
    "                batch_norm = int(block['batch_normalize'])\n",
    "            except:\n",
    "                batch_norm = 0\n",
    "            filters = int(block['filters'])\n",
    "            kernel_size = int(block['size'])\n",
    "            stride = int(block['stride'])\n",
    "            padding = int(block['pad'])\n",
    "            activation = block['activation']\n",
    "            # do padding or not\n",
    "            if padding:\n",
    "                padding = (kernel_size - 1) // 2\n",
    "            else:\n",
    "                padding = 0\n",
    "            # create the conv layer\n",
    "            module.add_module(\n",
    "                f\"conv_{layer_index}\",\n",
    "                nn.Conv2d(\n",
    "                    in_channels=filter_list[-1],\n",
    "                    out_channels=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding,\n",
    "                    bias=not batch_norm\n",
    "                )\n",
    "            )\n",
    "            # add batch_norm layer\n",
    "            if batch_norm:\n",
    "                module.add_module(\n",
    "                    f\"batch_nrom_{layer_index}\",\n",
    "                    nn.BatchNorm2d(filters)\n",
    "                )\n",
    "            if activation == 'leaky':\n",
    "                module.add_module(\n",
    "                    f\"leaky_{layer_index}\",\n",
    "                    nn.LeakyReLU(0.1, inplace=True)\n",
    "                )\n",
    "        # deal with maxpooling\n",
    "        elif block['Type'] == 'maxpool':\n",
    "            kernel_size = int(block['size'])\n",
    "            stride = int(block['stride'])\n",
    "            padding = (kernel_size - 1) // 2\n",
    "            if kernel_size is 2 and stride is 1:\n",
    "                module.add_module(\n",
    "                    f\"debug_padding_{layer_index}\",\n",
    "                    nn.ZeroPad2d((0,1,0,1))\n",
    "                )\n",
    "            module.add_module(\n",
    "                f\"maxpool_{layer_index}\",\n",
    "                nn.MaxPool2d(\n",
    "                    kernel_size=kernel_size,\n",
    "                    stride=stride,\n",
    "                    padding=padding\n",
    "                )\n",
    "            )\n",
    "        # deal with shortcuts (gradient flow, concate two outputs)\n",
    "        elif block['Type'] == 'shortcut':\n",
    "            # the output should not change through the shortcut in yolo, so no need to modify filters\n",
    "            module.add_module(\n",
    "                f\"shortcut_{layer_index}\",\n",
    "                EmptyLayer()\n",
    "            )\n",
    "        # deal with upsampling\n",
    "        elif block['Type'] == 'upsample':\n",
    "            scale_factor = int(block['stride'])\n",
    "            module.add_module(\n",
    "                f\"upsample_{layer_index}\",\n",
    "                nn.Upsample(scale_factor=scale_factor, mode='bilinear')\n",
    "            )\n",
    "        # deal with route layer (concate target output with previous layer's output [reconstruction])\n",
    "        elif block['Type'] == 'route':\n",
    "            tar_layers = block['layers'].split(',')\n",
    "            start_pos = int(tar_layers[0])\n",
    "            try:\n",
    "                end_pos = int(tar_layers[1])\n",
    "            except:\n",
    "                end_pos = 0\n",
    "            # thus we have end pos for it's not zero\n",
    "            # adjust the dimension of the current output\n",
    "            module.add_module(\n",
    "                f\"route_{layer_index}\",\n",
    "                EmptyLayer()\n",
    "            )\n",
    "            if end_pos != 0:\n",
    "                filters = filter_list[start_pos] + filter_list[end_pos]\n",
    "            else:\n",
    "                filters = filter_list[start_pos]\n",
    "        # deal with YoLo \n",
    "        elif block['Type'] == 'yolo':\n",
    "            anchor_idx = [int(idx) for idx in block['mask'].split(',')]\n",
    "            # extract all the numbers\n",
    "            anchors = [int(tag) for tag in block['anchors'].split(',')]\n",
    "            # group the anchor's (length, width) in one element\n",
    "            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors), 2)]\n",
    "            # pick out the selected (masked) elements\n",
    "            anchors = [anchors[i] for i in anchor_idx]\n",
    "            num_class = int(block['classes'])\n",
    "            img_size = (int(hyperparameters['height']), int(hyperparameters['width']))\n",
    "            ignore_thres = (float(block['ignore_thresh']))\n",
    "            module.add_module(\n",
    "                f\"yolo_v3_{layer_index}\",\n",
    "                YoLoLayer(anchors, num_class, img_size, ignore_thres)\n",
    "            )\n",
    "        # keep the current layer and dimension\n",
    "        module_list.append(module)\n",
    "        if filters:\n",
    "            filter_list.append(filters)\n",
    "    return hyperparameters, module_list"
   ]
  },
  {
   "source": [
    "## Darknet：YOLOv3模型构建\n",
    "\n",
    "### init：初始化\n",
    "\n",
    "存储指名使用的YOLO网络配置文件。\n",
    "\n",
    "### forward：前向传播\n",
    "\n",
    "将模型`Backbone`与`Head`的整体输入输出连接起来，计算输出与`loss`结果。<br>\n",
    "同时此处定义了`router`层与`shortcut`层，`router`层执行`concat`操作，将指定的两个`Layer`输出拼接<br>\n",
    "在`shortcut`中，利用上一层输出，将指定层输出直接加入其中（要保证维度对齐），构建梯度流。\n",
    "\n",
    "### load pretrained weights：载入训练参数\n",
    "\n",
    "根据指定文件载入权重数据，按层读取载入即可。\n",
    "\n",
    "### save weights：存储权重文件\n",
    "\n",
    "按层将当前训练结果参数进行存储。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Darknet(nn.Module):\n",
    "    '''\n",
    "    Handle the short cuts and routers\n",
    "    Load the pretrained weight if given\n",
    "    '''\n",
    "\n",
    "    def __init__(self, cfg_filepath, cuda=False):\n",
    "        super(Darknet, self).__init__()\n",
    "        self.cfg = cfg_filepath\n",
    "        self.cuda = cuda\n",
    "        self.blocks = parse_cfg(cfg_filepath)\n",
    "        self.params, self.module_list = generate_module(cfg_filepath)\n",
    "\n",
    "    def forward(self, x, targets=None, img_dim=None):\n",
    "        module_cfg = self.blocks[1:]\n",
    "        layer_outputs = []\n",
    "        outputs = []\n",
    "        loss = 0\n",
    "        for idx, module in enumerate(module_cfg):\n",
    "            if module['Type'] in ['convolutional', 'upsample', 'maxpool']:\n",
    "                # put the index in this layer to do the forward pass\n",
    "                x = self.module_list[idx](x)\n",
    "            # the logics of route and shortcut are defined here\n",
    "            elif module['Type'] == 'route':\n",
    "                tar_layers = [int(pos) for pos in module['layers'].split(',')]\n",
    "                if len(tar_layers) == 1:\n",
    "                    x = layer_outputs[tar_layers[0]]\n",
    "                else:\n",
    "                    x = torch.cat(\n",
    "                        (\n",
    "                            layer_outputs[tar_layers[0]],\n",
    "                            layer_outputs[tar_layers[1]]\n",
    "                        ),\n",
    "                        1\n",
    "                    )\n",
    "            elif module['Type'] == 'shortcut':\n",
    "                pos = int(module['from'])\n",
    "                x = layer_outputs[-1] + layer_outputs[pos]\n",
    "            # get the bounding box output and append it to the outputs\n",
    "            # we have serval output from different layers\n",
    "            elif module['Type'] == 'yolo':\n",
    "                x, layer_loss = self.module_list[idx][0](x, targets, img_dim)\n",
    "                outputs.append(x)\n",
    "                loss += layer_loss\n",
    "            layer_outputs.append(x)\n",
    "        # concate the outputs into one matrix\n",
    "        outputs = torch.cat(outputs, 1)\n",
    "        return outputs if targets is None else (outputs, loss)\n",
    "\n",
    "    def load_pretrained_weights(self, weight_filepath):\n",
    "        with open(weight_filepath, 'rb') as f:\n",
    "            # get the values of the header\n",
    "            header = np.fromfile(f, dtype=np.int32, count=5)\n",
    "            # 1. Major version number\n",
    "            # 2. Minor Version Number\n",
    "            # 3. Subversion number \n",
    "            # 4,5. Images seen by the network (during training)\n",
    "            self.seen = header[3]\n",
    "            # needed when saving new weight files\n",
    "            self.header_info = header\n",
    "            weights = np.fromfile(f, dtype=np.float32)\n",
    "        # start to write weights into the backbone\n",
    "        module_cfg = self.blocks[1:]\n",
    "        ptr = 0\n",
    "        for idx, module in enumerate(module_cfg):\n",
    "            # we need to load weights to all the conv layers\n",
    "            if module['Type'] == 'convolutional':\n",
    "                layer = self.module_list[idx]\n",
    "                try:\n",
    "                    batch_norm = int(module['batch_normalize'])\n",
    "                except:\n",
    "                    batch_norm = 0\n",
    "                # get the conv layer\n",
    "                conv_layer = layer[0]\n",
    "                if batch_norm:\n",
    "                    bn_layer = layer[1]\n",
    "                    # number of params\n",
    "                    num_bias = bn_layer.bias.numel()\n",
    "                    # load the bias (always remember to reshpe the matrix into proper form)\n",
    "                    bn_bias = torch.from_numpy(weights[ptr : ptr + num_bias]).view_as(bn_layer.bias)\n",
    "                    bn_layer.bias.data.copy_(bn_bias)\n",
    "                    ptr += num_bias\n",
    "                    # load the weight\n",
    "                    bn_weight = torch.from_numpy(weights[ptr : ptr + num_bias]).view_as(bn_layer.weight)\n",
    "                    bn_layer.weight.data.copy_(bn_weight)\n",
    "                    ptr += num_bias\n",
    "                    # load the running mean\n",
    "                    bn_running_mean = torch.from_numpy(weights[ptr : ptr + num_bias]).view_as(bn_layer.running_mean)\n",
    "                    bn_layer.running_mean.data.copy_(bn_running_mean)\n",
    "                    ptr += num_bias\n",
    "                    # load the running varianve\n",
    "                    bn_running_var = torch.from_numpy(weights[ptr : ptr + num_bias]).view_as(bn_layer.running_var)\n",
    "                    bn_layer.running_var.data.copy_(bn_running_var)\n",
    "                    ptr += num_bias\n",
    "                else:\n",
    "                    # load the conv bias\n",
    "                    num_bias = conv_layer.bias.numel()\n",
    "                    conv_bias = torch.from_numpy(weights[ptr : ptr + num_bias]).view_as(conv_layer.bias)\n",
    "                    conv_layer.bias.data.copy_(conv_bias)\n",
    "                    ptr += num_bias\n",
    "                # load the conv weights\n",
    "                num_weight = conv_layer.weight.numel()\n",
    "                conv_weight = torch.from_numpy(weights[ptr : ptr + num_weight]).view_as(conv_layer.weight)\n",
    "                conv_layer.weight.data.copy_(conv_weight)\n",
    "                ptr += num_weight\n",
    "                \n",
    "    def save_weights(self, target_filepath):\n",
    "        save_file = open(target_filepath, 'wb')\n",
    "        self.header_info[3] = self.seen\n",
    "        self.header_info.tofile(save_file)\n",
    "        # Iterate to save all layers\n",
    "        module_cfg = self.blocks[1:]\n",
    "        for idx, module in enumerate(module_cfg):\n",
    "            # also, just need to keep all the conv layer's weights\n",
    "            if module['Type'] == 'convolutional':\n",
    "                conv_layer = self.module_list[idx][0]\n",
    "                try:\n",
    "                    batch_norm = int(module['batch_normalize'])\n",
    "                except:\n",
    "                    batch_norm = 0\n",
    "                if batch_norm:\n",
    "                    bn_layer = self.module_list[idx][1]\n",
    "                    bn_layer.bias.data.cpu().numpy().tofile(save_file)\n",
    "                    bn_layer.weight.data.cpu().numpy().tofile(save_file)\n",
    "                    bn_layer.running_mean.data.cpu().numpy().tofile(save_file)\n",
    "                    bn_layer.running_var.data.cpu().numpy().tofile(save_file)\n",
    "                else:\n",
    "                    conv_layer.bias.data.cpu().numpy().tofile(save_file)\n",
    "                conv_layer.weight.data.cpu().numpy().tofile(save_file)\n",
    "        save_file.close()"
   ]
  },
  {
   "source": [
    "### YOLO模型构建使用\n",
    "\n",
    "- weight init：初始化网络层权重，针对`Conv`层与`BatchNorm`层的参数进行服从正态分布的参数初始化。\n",
    "\n",
    "- Hyperparameters：一般来说超参数定义在`obj.data`，`yolo-obj.cfg`与`obj.names`数个文件中，记录当前模型与学习任务的各项相关参数。\n",
    "    \n",
    "    - 网络模型参数如`学习率`，`迭代次数`，`batch尺寸`，`输入数据维度`等，在`yolo-obj.cfg`文件中标明。\n",
    "\n",
    "    - 训练数据属性如目标类别数，训练样本列表定义文件目录，验证/测试样本列表定义文件目录，各类别名定义文件目录以及模型参数文件自动存储目录，在`obj.data`文件中明确。\n",
    "\n",
    "    - `obj.names`文件明确各类别的对应名字；`train.txt`明确所有训练样本路径；`test.txt`明确所有测试样本路径。\n",
    "\n",
    "- Dataloader：读入YOLO格式的批量图片文件，并送入网络进行前向传播预测。\n",
    "\n",
    "    - 需要为YOLO格式数据定义新Dataset，以满足对文件记录路径上图片的`读取`，`形状转化`和`padding`等操作。\n",
    "    \n",
    "    - 针对训练的Dataset还需要进一步修改，做到能够同时读取与图片文件同名的标记txt文件内的具体`bounding box`内容。\n",
    "\n",
    "- 批量结果测试与训练简单示例。\n",
    "\n",
    "    - 可以在训练使用的Dataset定义中添加`collate_fn`函数，即可用户自定义batch训练的相关处理机制。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we directly define some parameters we need here\n",
    "class_num = 2\n",
    "train = \"[train_file_path]\"\n",
    "valid = \"[test_file_path]\"\n",
    "backup = \"[model_backup_path]\"\n",
    "\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "img_width = 416\n",
    "img_height = 416\n",
    "epoch = 2000\n",
    "gradient_accumulations = 2\n",
    "checkpoint_interval = 100\n",
    "\n",
    "cfg_filepath = \"[cfg_path]/yolov3.cfg\"\n",
    "weight_filepath = '[weight_path]/yolov3.weights'\n",
    "\n",
    "def weight_init(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.nromal_(model.weight.data, 0.0, 0.02)\n",
    "     elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(model.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(model.bias.data, 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create model\n",
    "model = Darknet(cfg_filepath=cfg_filepath)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since we have the pre-trained weights file, we do not need to init the weight.\n",
    "model.load_pretrained_weights(weight_filepath)\n",
    "# get into evaluation mode, thus some regularzation layers will be disabled.\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand the dataset to read YOLO format training data\r\n",
    "class ImgDataset(Dataset):\r\n",
    "\r\n",
    "    def __init__(self, folder_path, img_size=(416,416)):\r\n",
    "        self.files = sorted(glob.glob(\"%s/*.*\" % folder_path))\r\n",
    "        self.img_size = img_size[0]\r\n",
    "\r\n",
    "    def __getitem__(self, index):\r\n",
    "        # get the current image the index points to\r\n",
    "        img_path = self.files[index % len(self.files)]\r\n",
    "        img = transforms.ToTensor()(Image.open(img_path))\r\n",
    "        # if the training data is not fixed with its size (not 416x416 square pictures), we need to pad them to square pictures with 0s\r\n",
    "        # resize the image to 416x416x3\r\n",
    "        img = F.interpolate(img.unsqueeze(0), size=self.size, mode=\"nearest\").squeeze(0)\r\n",
    "        return img_path, img\r\n",
    "\r\n",
    "    def __len__(self):\r\n",
    "        return len(self.files)\r\n",
    "\r\n",
    "# read the pictures with the dataloader\r\n",
    "dataloader = DataLoader(\r\n",
    "    ImgDataset(folder_path=valid, img_size=(img_width, img_height)),\r\n",
    "    batch_size=batch_size,\r\n",
    "    shuffle=False,\r\n",
    "    num_workers=num_workers\r\n",
    ")\r\n",
    "\r\n",
    "# read in the class names (should be defined in obj.names file)\r\n",
    "class_names = [\"cls_1\", \"cls_2\"]\r\n",
    "\r\n",
    "# do the prediction\r\n",
    "img_detected = []\r\n",
    "img_predictions = []\r\n",
    "for batch_i, (img_paths, imgs) in enumerate(dataloader):\r\n",
    "    # adjust the configuration of the images\r\n",
    "    imgs = Variable(imgs.type(Tensor))\r\n",
    "    # get the predictions\r\n",
    "    with torch.no_grad():\r\n",
    "        detections = model(imgs)\r\n",
    "        # then, we should have an nms layer that filter all the bounding boxes that have low confidence, this time we skip this layer\r\n",
    "    img_detected.extend(imgs)\r\n",
    "    img_predictions.extend(detections)\r\n",
    "\r\n",
    "# show the results of the bounding boxes\r\n",
    "for img_i, (img_path, detection) in enumerate(zip(img_detected, img_predictions)):\r\n",
    "    if detection is not None:\r\n",
    "        # we have to resize the bounding boxes according to the original picture size, currently we assume the pictures are all 416x416 without any padding.\r\n",
    "        for x, y, w, h, conf, cls_conf, cls_pred in detection:\r\n",
    "            print(\"\\tScanned image: %s  Found object: %s at (%.5f, %.5f, %.5f, %.5f) with conf %.5f\" % (img_path, class_names[int(cls_pred)], x, y, w, h, cls_conf.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset for training, it needs to output the images and their labels\n",
    "class TrainDataset(Dataset):\n",
    "\n",
    "    def __init__(self, file_path, img_size=(416, 416)):\n",
    "        # read in all the image paths\n",
    "        with open(file_path, \"r\") as file:\n",
    "            self.img_files = file.readlines()\n",
    "        # read in all the label files\n",
    "        self.label_files = [path.replace(\"images\", \"labels\").replace(\".png\", \".txt\").replace(\".jpg\", \".txt\") for path in self.img_files]\n",
    "        self.img_size = img_size[0]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_files[index % len(self.img_files)].rstrip()\n",
    "        img = transforms.ToTensor()(Image.open(img_path).convert('RGB'))\n",
    "        # we assume our pictures are all 416x416x3 and all the labels are normalised.\n",
    "        label_path = self.label_files[index % len(self.img_files)].rstrip()\n",
    "        boxes = torch.from_numpy(np.loadtxt(label_path).reshape(-1, 5))\n",
    "        # extract the targets from the boxes\n",
    "        targets = torch.zeros((len(boxes), 6))\n",
    "        targets[:, 1:] = boxes\n",
    "        return img_path, img, targets\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "dataloader_train = DataLoader(\n",
    "    ListDataset(file_path=train, img_size=(img_width, img_height)),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "# here we use Adam optimizer with default settings\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "# start training\n",
    "for epoch_i in range(epoch):\n",
    "    model.train()\n",
    "    # this dataloader is different from the previous one used in the prediction, it needs to return the labels from .txt files\n",
    "    for batch_i, (_, imgs, labels) in enumerate(dataloader_train):\n",
    "        batch_done = len(dataloader_train) * epoch_i + batch_i\n",
    "        # adjust the formats\n",
    "        imgs = Variable(imgs)\n",
    "        targets = Variable(targets, requires_grad=False)\n",
    "        # do the forward and gather the loss\n",
    "        loss, outputs = model(imgs, targets)\n",
    "        loss.backward()\n",
    "        # do the backpropagation\n",
    "        if batches_done % opt.gradient_accumulations == 1:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "    # save the current model weights\n",
    "    if epoch_i >= checkpoint_interval and epoch_i % checkpoint_interval == 0:\n",
    "        torch.save(model.state_dict(), f\"%s/yolov3_ckpt_%d.pth\" % (backup, epoch))"
   ]
  },
  {
   "source": [
    "### 参考资料\n",
    "\n",
    "- [Referrence 1](https://blog.paperspace.com/how-to-implement-a-yolo-v3-object-detector-from-scratch-in-pytorch-part-2/)\n",
    "\n",
    "- [Referrence 2](https://github.com/eriklindernoren/PyTorch-YOLOv3)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}