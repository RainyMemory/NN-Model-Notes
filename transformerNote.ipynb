{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Transformer Note\n",
    "- 代码完全参考[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "- 仅补充部分个人理解与分析"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math, copy, time\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cloneModel(module, N):\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "source": [
    "## 模型本身构成分析\n",
    "- 定义使用的`LayerNorm`函数，将输入归一化后映射处理\n",
    "- 默认无偏，故`bias`全0"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, featureNum, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.mapping = nn.Parameter(torch.ones(featureNum))\n",
    "        self.bias = nn.Parameter(torch.zeros(featureNum))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        # mapping矩全1，保留去中心化后的输出结果，利用eps参数控制基础分布散度，并在最后补入偏差bias（全0）\n",
    "        return self.mapping * (x - mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "source": [
    "- 包裹`Attention`和`LayerNorm`两层，将通过`LayerNorm`处理后的输入经由指定网络层`sublayer`并在最后讲结果与原输入拼接（整个过程中`Embedding`的维度不发生变化）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, featureNum, dropout):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.norm = LayerNorm(featureNum)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, sublayer):\n",
    "        # 注意，此处需要给出的sublayer在处理数据时，不对数据维度进行扰动，以确保输出结果矩阵能够满足Residual求和\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "source": [
    "- 进行Encode处理，每穿过`Attention`层后进行一次`Feedforward`，采用残差链接的方式\n",
    "- 传入的`x`即为`key`，`query`以及`value`,注意三者会经过三个不同的Linear层进行映射，以来进行`self-attention`的相关计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        # Query,Key,Value三者通过多层Multihead-Attention进行注意力捕捉\n",
    "        self.layers = cloneModel(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, featureNum, selfAttn, feedforward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.selfAttn = selfAttn\n",
    "        self.feedforward = feedforward\n",
    "        # 两个Residual层，分别用于Attention和Feedforward层的梯度流构建\n",
    "        self.sublayer = cloneModel(ResidualConnection(featureNum, dropout), 2)\n",
    "        self.featureNum = featureNum\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Sel-multihead-attention捕捉分析流程（用自身作为Key，Query，Value进行Attention关注训练）\n",
    "        x = self.sublayer[0](x, lambda x: self.selfAttn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feedforward)"
   ]
  },
  {
   "source": [
    "- Mask的创建操作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsquentMask(featureNum):\n",
    "    attnShape = (1, featureNum, featureNum)\n",
    "    # np.triu()返回了给定形状矩阵的上三角矩阵，在Mask步骤中，我们的目的是屏蔽未见的未来信息，利用三角矩阵特性，对输入信息进行padding\n",
    "    subsquentMask = np.triu(np.ones(attnShape), k=1).astype('uint8')\n",
    "    return torch.from_numpy(subsquentMask) == 0"
   ]
  },
  {
   "source": [
    "- 每个`Attention`的定义，利用矩阵相乘求取所得的`Attention`会经由`Softmax`函数进行进一步拉伸\n",
    "- $Softmax(Q·K^T)·V/\\sqrt{Dim}$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    dimension = query.size(-1)\n",
    "    attnScore = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(dimension)\n",
    "    if mask is not None:\n",
    "        # 纯0项不能帮助Softmax进一步扭曲分布状态\n",
    "        attnScore = attnScore.masked_fill(mask == 0, -1e9)\n",
    "    twsAttn = F.softmax(attnScore, dim=-1)\n",
    "    if dropout is not None:\n",
    "        twsAttn = dropout(twsAttn)\n",
    "    # 输出内容为：当前Attention关注的原文内容，以及Attention计算结果本身\n",
    "    return torch.matmul(twsAttn, value), twsAttn"
   ]
  },
  {
   "source": [
    "- 四个`Linear`层中的前三个分别对`key`，`query`，`value`进行线性映射，调整其`Embedding`后计算`Attention`，最后一层`Linear`用于对`Attention`的计算结果进行线性映射"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, headNum, dimension, dropout=0.1):\n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        self.vocabDim = dimension // headNum\n",
    "        self.headNum = headNum\n",
    "        self.linears = cloneModel(nn.Linear(dimension, dimension), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key ,value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        batchSize = query.size(0)\n",
    "        # Query，Key，Value三个值通过Linear层进行映射，做输入Embedding调整\n",
    "        query, key, value = [linear(x).view(batchSize, -1, self.headNum, self.vocabDim) for linear, x in zip(self.linears, (query, key, value))]\n",
    "        # 调用Attention计算，将Linear变换后的三者作为输入进行Self-attention训练，此处x所得即为本输入经由Attention捕捉的关注内容\n",
    "        x, self.attn = attention(query, key, value, mask, self.dropout)\n",
    "        x = x.transpose(1, 2).contiguous().view(batchSize, -1, self.headNum, self.vocabDim)\n",
    "        # 最后将当前关注内容在进行依次Liner映射，进行最终调整并输出\n",
    "        return self.linears[-1](x)"
   ]
  },
  {
   "source": [
    "- `Feedforward`层，将所得的`Embedding`进行一次维度变化和还原来扰动和调整输出结果"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, dimension, feedforwardDim, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(dimension, feedforwardDim)\n",
    "        self.linear2 = nn.Linear(feedforwardDim, dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过两个Linear层进行维度的折叠扰动（类似DarkNet的维度折叠展开块）\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x"
   ]
  },
  {
   "source": [
    "- `Embedding`层，将词映射到指定维度的词向量"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, dimension, vocab):\n",
    "        super(Embedding, self).__init__()\n",
    "        # 获取pre-trained词向量\n",
    "        self.embed = nn.Embedding(vocab, dimension)\n",
    "        self.dimension = dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embed(x) * math.sqrt(self.dimension)"
   ]
  },
  {
   "source": [
    "- 对于位置编码，其要求在指定的`Sequence`长度范围内，能够衡量两两`token`间的位置距离且各`token`的位置表示独特唯一，并且值位于`[-1, 1]`区间之内\n",
    "- 采用$PosEncode_{(position, 2*embedDim)} = Sin(position/10000^{2*embedDim/dimension})$对位置进行编码计算"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEncoding(nn.Module):\n",
    "    def __init__(self, dimension, dropout, maxSeqLen=5000):\n",
    "        super(PosEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        posEncode = torch.zeros(maxSeqLen, dimension)\n",
    "        position = torch.arange(0, maxSeqLen).unsqueeze(1)\n",
    "        divTerm = torch.exp(torch.arange(0, dimension, 2) * -(math.log(10000.0) / dimension))\n",
    "        posEncode = posEncode.unsqueeze(0)\n",
    "        # 进行Position Encoding操作\n",
    "        posEncode[:, 0::2] = torch.sin(position * divTerm)\n",
    "        posEncode[:, 1::2] = torch.cos(position * divTerm)\n",
    "        self.registerBuffer('posEncode', posEncode)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + Variable(self.posEncode[:, :x.size(1)], require_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "source": [
    "- `Decoder`部分，用于进行解码工作，将Encode的结果，根据网络记忆内容（能看到之前时刻的，不超过最大Sequence限长的累计输入），根据原先的Mask映射到原本输入上"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        # 与Encoder同理，为多层Multihead-Attention复合结构\n",
    "        self.layers = cloneModel(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "    \n",
    "    def forward(self, x, memory, srcMask, tarMask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, srcMask, tarMask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, featureNum, selfAttn, srcAttn, feedforward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.featureNum = featureNum\n",
    "        self.selfAttn = selfAttn\n",
    "        # 用于解码，通过Encoder提供的Key，Query和自身的Self-Attention提供的Value进行Attention捕捉训练\n",
    "        self.srcAttn = srcAttn\n",
    "        self.feedforward = feedforward\n",
    "        self.sublayer = cloneModel(ResidualConnection(featureNum, dropout), 3)\n",
    "\n",
    "    def forward(self, x, memory, srcMask, tarMask):\n",
    "        memo = memory\n",
    "        # 获取Self-Attention输出，作为最终的Value使用\n",
    "        x = self.sublayer[0](x, lambda x: self.selfAttn(x, x, x, tarMask))\n",
    "        # memo记录Encoder的提供输入，即Decoder的输出，作为Key与Value使用，同时，由于输入来自于Encoder，需要利用srcMask进行Mask操作\n",
    "        x = self.sublayer[1](x, lambda x: self.srcAttn(x, memo, memo, srcMask))\n",
    "        return self.sublayer[2](x, self.feedforward)"
   ]
  },
  {
   "source": [
    "- 整体`Transformer`的训练流程模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, srcEmbed, tarEmbed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.srcEmbed = srcEmbed\n",
    "        self.tarEmbed = tarEmbed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, src, tar, srcMask, tarMask):\n",
    "        # 完整的Encode，Decode流程：Encode的结果是条件，Decode的结果是目标\n",
    "        encode = self.encode(src, srcMask)\n",
    "        decode = self.decode(encode, srcMask, tar, tarMask)\n",
    "        return decode\n",
    "    \n",
    "    def encode(self, src, srcMask):\n",
    "        return self.encoder(self.srcEmbed(src), srcMask)\n",
    "\n",
    "    def decode(self, memory, srcMask, tar, tarMask):\n",
    "        return self.decoder(self.tarEmbed(tar), memory, srcMask, tarMask)"
   ]
  },
  {
   "source": [
    "- 最终输出的生成，进行向输入词向量维度的还原映射工作"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, dimension, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        # 利用Linear层向目标词向量维度变换\n",
    "        self.linear = nn.Linear(dimension, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 利用Softmax进行概率化输出\n",
    "        return F.log_softmax(self.linear(x), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(srcVocab, tarVocab, N=6, dimension=512, feedforwardDim=2048, headNum=8, dropout=0.1):\n",
    "    cp = copy.deepcopy\n",
    "    # Multihead-Attention，Feedforward， Position-Encoding三层准备\n",
    "    attentions = MultiheadAttention(headNum, dimension)\n",
    "    feedforward = PositionwiseFeedForward(dimension, feedforwardDim, dropout)\n",
    "    position = PosEncoding(dimension, dropout)\n",
    "    # 构建模型本体：加码，解码并转化到词向量各维概率输出\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(dimension, cp(attentions), cp(feedforward), dropout), N),\n",
    "        Decoder(DecoderLayer(dimension, cp(attentions), cp(attentions), cp(feedforward), dropout), N),\n",
    "        nn.Sequential(Embedding(dimension, srcVocab), cp(position)),\n",
    "        nn.Sequential(Embedding(dimension, tarVocab), cp(position)),\n",
    "        Generator(dimension, tarVocab)\n",
    "    )\n",
    "    # 参数初始化，使其服从xavier型式，加速训练梯度流向\n",
    "    for param in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform(p)\n",
    "    return model"
   ]
  },
  {
   "source": [
    "## 模型训练部分函数分析\n",
    "- 此处mask对输入信息进行限制，即在按顺序读取的假设前提下，不会知道在当前输入之后的信息\n",
    "- 进行mask后能够屏蔽后续输入的原因可以[查看此处](https://blog.csdn.net/qq_35169059/article/details/101678207)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    def __init__(self, src, tar=None, padding=0):\n",
    "        # 源输入封装如src下属性\n",
    "        self.src = src\n",
    "        self.srcMask = (src != padding).unsqueeze(-2)\n",
    "        if tar is not None:\n",
    "            # tarX为当前处理内容，tarY为其之后一位内容，即当前任务的下一位，作为目标\n",
    "            self.tarX = tar[:, :-1]\n",
    "            self.tarY = tar[:, 1:]\n",
    "            self.tarMask = self.makeStdMask(self.tarX, padding)\n",
    "            self.ntokens = (self.tarY != padding).data.sum()\n",
    "        \n",
    "    @staticmethod\n",
    "    def makeStdMask(target, padding):\n",
    "        targetMask = (target != padding).unsqueeze(-2)\n",
    "        targetMask = targetMask & Variable(subsquentMask(target.size(-1)).type_as(targetMask.data))\n",
    "        return targetMask"
   ]
  },
  {
   "source": [
    "- 对采用的`Adam`优化方法在训练时进行进一步的学习率管理，使其更适用于Transformer的训练调优"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOptimizer:\n",
    "    def __init__(self, featureNum, factor, warmup, optimizer):\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.featureNum = featureNum\n",
    "        self._rate = 0\n",
    "\n",
    "    # 在每次更新数据后，渐进式的调整学习率等参数\n",
    "    def step(self):\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for param in self.optimizer.param_groups:\n",
    "            param['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "    \n",
    "    def rate(self, step=None):\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.featureNum ** (-0.5) * min(step ** (-0.5), step * (self.warmup) ** (-1.5)))\n",
    "\n",
    "def getOptimizer(model):\n",
    "    return NoamOptimizer(model.srcEmbed[0].dimension, 2, 4000, torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "source": [
    "- 修改后的`Adam`优化器产生的`Loss`将在任务特定的Loss计算中被向后传播，进行参数优化"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCompute:\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        # 用于进行输出维度调整，将其与目标格式/含义对齐\n",
    "        self.generator = generator\n",
    "        # Loss自定义计算标准，将模型输出值列表x，与真实标记列表y进行对比求取总Loss\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "    \n",
    "    def __call__(self, x, y, norm):\n",
    "        x = self.generator(x)\n",
    "        # Norm一般为BatchSize，即当前一个批次处理了多少数据样本\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)), y.contiguous().view(-1)) / norm\n",
    "        # 在此处启动梯度回溯，进行参数更新\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            self.opt.step()\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        return loss.data[0] * norm"
   ]
  },
  {
   "source": [
    "- 故我们每次训练的`Epoch`执行逻辑如下。"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runEpoch(dataIter, model, lossCompute):\n",
    "    totalTokens = 0\n",
    "    totalLoss = 0\n",
    "    for i, batch in enumerate(dataIter):\n",
    "        output = model.forward(batch.src, batch.tarX, batch.srcMask, batch.tarMask)\n",
    "        # 注意模型output与目标输出TargetY并不一定维度对应，传入的lossCompute函数需要去定义output的转化方案和loss的计算准则\n",
    "        loss = lossCompute(output, batch.tarY, batch.ntokens)\n",
    "        totalLoss += loss\n",
    "        totalTokens += batch.ntokens\n",
    "        if i % 100 == 1:\n",
    "            print(\"Epoch Step: %d Loss: %f\" %(i, loss / batch.ntokens))\n",
    "    return totalLoss / totalTokens"
   ]
  }
 ]
}